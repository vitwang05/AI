import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
from langchain.schema import Document
from app.config import *
import fitz  # PyMuPDF
import re
import json

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    length_function=len,
)

embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
def chunk_articles_with_metadata(text, document_name="VÄƒn báº£n phÃ¡p luáº­t"):
    def split_into_chapters(text):
        # Náº¿u khÃ´ng cÃ³ chÆ°Æ¡ng, tráº£ vá» má»™t chÆ°Æ¡ng giáº£
        if not re.search(r'ChÆ°Æ¡ng\s+[IVXLCDM]+\.', text):
            return [("KhÃ´ng cÃ³ chÆ°Æ¡ng", text.strip())]

        # TÃ¡ch theo chÆ°Æ¡ng
        chapters = re.split(r'(ChÆ°Æ¡ng\s+[IVXLCDM]+\.\s+.+)', text)
        result = []
        for i in range(1, len(chapters), 2):
            chapter_title = chapters[i].strip()
            chapter_content = chapters[i + 1].strip()
            result.append((chapter_title, chapter_content))
        return result

    def split_into_articles(chapter_content):
        # TÃ¡ch theo Ä‘iá»u
        articles = re.split(r'(Äiá»u\s+\d+\.\s+.+)', chapter_content)
        result = []
        for i in range(1, len(articles), 2):
            article_title = articles[i].strip()
            article_content = articles[i + 1].strip()
            full_article = article_title + "\n" + article_content
            result.append((article_title, full_article))
        return result

    chunks_with_metadata = []
    chapters = split_into_chapters(text)

    for chapter_title, chapter_content in chapters:
        articles = split_into_articles(chapter_content)
        for article_title, full_article in articles:
            chunks = text_splitter.split_text(full_article)
            for chunk in chunks:
                chunks_with_metadata.append({
                    "text": chunk,
                    "metadata": {
                        "program": document_name,
                        "chapter_title": chapter_title,
                        "article_title": article_title,
                        "article_number": re.search(r'Äiá»u\s+(\d+)', article_title).group(1) if re.search(r'Äiá»u\s+(\d+)', article_title) else None
                    }
                })

    return chunks_with_metadata
    
def setup_pinecone_index():
    pc = Pinecone(api_key=PINECONE_API_KEY)
    if PINECONE_INDEX_NAME not in pc.list_indexes().names():
        pc.create_index(
            name=PINECONE_INDEX_NAME,
            dimension=768,
            metric='cosine',
            spec=ServerlessSpec(cloud='aws', region='us-east-1')
        )
    return pc

def process_document(file_path):
    print(f"ğŸ“„ Äang xá»­ lÃ½ file: {file_path}")

    # Load ná»™i dung file
    loader = Docx2txtLoader(file_path)
    documents = loader.load()

    # GhÃ©p láº¡i ná»™i dung thÃ nh chuá»—i lá»›n
    full_text = "\n".join([doc.page_content for doc in documents])

    # TÃ¡ch chÆ°Æ¡ng > Ä‘iá»u > chunk nhá»
    splits = []
    chapter_article_chunks = chunk_articles_with_metadata(full_text, document_name=os.path.basename(file_path))

    for chunk in chapter_article_chunks:
        splits.append(Document(
            page_content=chunk["text"],
            metadata=chunk["metadata"]
        ))

    # ÄÆ°a vÃ o Pinecone
    vectorstore = PineconeVectorStore.from_documents(
        documents=splits,
        embedding=embeddings,
        index_name=PINECONE_INDEX_NAME,
        pinecone_api_key=PINECONE_API_KEY
    )

    print(f"âœ… ÄÃ£ lÆ°u {len(splits)} chunks vÃ o Pinecone")
    return vectorstore


import re
import fitz  # PyMuPDF
# import docx

def extract_structured_terms(file_path, start_page, end_page):
    try:
        # Má»Ÿ file dá»±a theo Ä‘á»‹nh dáº¡ng
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            if not doc:
                raise ValueError("Could not open PDF file")
            total_pages = len(doc)
            print(f"Total pages in PDF: {total_pages}")
        elif file_path.lower().endswith('.docx'):
            doc = Document(file_path)
            total_pages = len(doc.paragraphs)
            print(f"Total paragraphs in DOCX: {total_pages}")
        else:
            raise ValueError("Unsupported file format. Only PDF and DOCX files are supported.")

        # Kiá»ƒm tra pháº¡m vi trang/Ä‘oáº¡n vÄƒn
        if start_page < 1 or start_page > total_pages:
            raise ValueError(f"Start page {start_page} is out of range (1-{total_pages})")
        if end_page < start_page or end_page > total_pages:
            raise ValueError(f"End page {end_page} is out of range ({start_page}-{total_pages})")

        text = ""

        # TrÃ­ch xuáº¥t text tá»« PDF
        if file_path.lower().endswith('.pdf'):
            for i in range(start_page - 1, end_page):
                try:
                    page = doc[i]
                    if page:
                        page_text = page.get_text()
                        if page_text:
                            text += page_text + "\n"
                            print(f"Successfully extracted text from page {i+1}")
                        else:
                            print(f"Warning: No text found on page {i+1}")
                    else:
                        print(f"Warning: Could not access page {i+1}")
                except Exception as e:
                    print(f"Error processing page {i+1}: {str(e)}")
        # TrÃ­ch xuáº¥t text tá»« DOCX
        else:
            for i in range(start_page - 1, end_page):
                try:
                    para_text = doc.paragraphs[i].text
                    if para_text:
                        text += para_text + "\n"
                        print(f"Successfully extracted text from paragraph {i+1}")
                    else:
                        print(f"Warning: No text found in paragraph {i+1}")
                except Exception as e:
                    print(f"Error processing paragraph {i+1}: {str(e)}")

        if not text.strip():
            raise ValueError("No text content found in the specified page range")

        print(f"Extracted text length: {len(text)} characters")
        lines = text.splitlines()
        print(f"Number of lines: {len(lines)}")

        result = []
        current_term = None
        current_sub = None
        current_detail = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Äiá»u khoáº£n chÃ­nh: "Äiá»u 1:", "Äiá»u 2."
            if re.match(r"^Äiá»u\s+\d+[\.:]", line):
                # Káº¿t thÃºc khá»‘i cÅ©
                if current_term:
                    if current_sub:
                        if current_detail:
                            current_sub["details"].append(current_detail)
                            current_detail = None
                        current_term["sub_items"].append(current_sub)
                        current_sub = None
                    result.append(current_term)
                current_term = {"title": line, "sub_items": []}
                current_sub = None
                current_detail = None

            # Má»¥c 1.1, 2.3,...
            elif re.match(r"^\d+\.\d+", line):
                if current_term is None:
                    # Khá»Ÿi táº¡o current_term máº·c Ä‘á»‹nh náº¿u chÆ°a cÃ³
                    current_term = {"title": "KhÃ´ng rÃµ tiÃªu Ä‘á»", "sub_items": []}

                if current_sub:
                    if current_detail:
                        current_sub["details"].append(current_detail)
                        current_detail = None
                    current_term["sub_items"].append(current_sub)
                current_sub = {"title": line, "details": []}
                current_detail = None

            # Má»¥c a), b), c)...
            elif re.match(r"^[a-zA-Z]\)", line):
                if current_term is None:
                    current_term = {"title": "KhÃ´ng rÃµ tiÃªu Ä‘á»", "sub_items": []}

                if current_detail:
                    current_sub["details"].append(current_detail)
                current_detail = {"title": line, "sub_details": []}

            # Má»¥c i), ii), iii)...
            elif re.match(r"^(i{1,3}|iv|v|vi|vii|viii|ix|x)\)", line, re.IGNORECASE):
                if current_detail:
                    current_detail["sub_details"].append(line)
                elif current_sub:
                    current_sub["details"].append({"title": line})

            # Má»¥c - gáº¡ch Ä‘áº§u dÃ²ng
            elif re.match(r"^- ", line):
                if current_detail:
                    current_detail["sub_details"].append(line)
                elif current_sub:
                    current_sub["details"].append({"title": line})

            # DÃ²ng ná»‘i tiáº¿p
            else:
                # Náº¿u Ä‘ang á»Ÿ cáº¥p sÃ¢u nháº¥t (i) hoáº·c -), ná»‘i vÃ o dÃ²ng cuá»‘i
                if current_detail and current_detail.get("sub_details"):
                    current_detail["sub_details"][-1] += " " + line
                # Náº¿u Ä‘ang á»Ÿ cáº¥p a)
                elif current_detail:
                    current_detail["title"] += " " + line
                # Náº¿u chá»‰ cÃ³ cáº¥p 1.1
                elif current_sub:
                    current_sub["title"] += " " + line
                else:
                    # KhÃ´ng cÃ³ cáº¥p nÃ o - cÃ³ thá»ƒ lÆ°u thÃ nh term riÃªng hoáº·c bá» qua
                    if current_term is None:
                        current_term = {"title": "KhÃ´ng rÃµ tiÃªu Ä‘á»", "sub_items": []}
                    # ThÃªm dÃ²ng nÃ y vÃ o title cá»§a current_term (hoáº·c táº¡o 1 sub má»›i)
                    # á» Ä‘Ã¢y mÃ¬nh chá»n ná»‘i vÃ o title Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
                    current_term["title"] += " " + line

        # ÄÃ³ng cÃ¡c khá»‘i cÃ²n láº¡i
        if current_detail:
            current_sub["details"].append(current_detail)
        if current_sub:
            current_term["sub_items"].append(current_sub)
        if current_term:
            result.append(current_term)

        if not result:
            raise ValueError("No structured content found in the text")

        print(f"Found {len(result)} terms in the text")
        return result

    except Exception as e:
        raise Exception(f"Error processing file: {str(e)}")

    try:
        # Open document based on file extension
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            if not doc:
                raise ValueError("Could not open PDF file")
            total_pages = len(doc)
            print(f"Total pages in PDF: {total_pages}")
        elif file_path.lower().endswith('.docx'):
            doc = Document(file_path)
            total_pages = len(doc.paragraphs)
            print(f"Total paragraphs in DOCX: {total_pages}")
        else:
            raise ValueError("Unsupported file format. Only PDF and DOCX files are supported.")

        # Validate page range
        if start_page < 1 or start_page > total_pages:
            raise ValueError(f"Start page {start_page} is out of range (1-{total_pages})")
        if end_page < start_page or end_page > total_pages:
            raise ValueError(f"End page {end_page} is out of range ({start_page}-{total_pages})")

        text = ""
        
        # Extract text from PDF
        if file_path.lower().endswith('.pdf'):
            for i in range(start_page - 1, end_page):
                try:
                    page = doc[i]
                    if page:
                        page_text = page.get_text()
                        if page_text:
                            text += page_text + "\n"
                            print(f"Successfully extracted text from page {i+1}")
                        else:
                            print(f"Warning: No text found on page {i+1}")
                    else:
                        print(f"Warning: Could not access page {i+1}")
                except Exception as e:
                    print(f"Error processing page {i+1}: {str(e)}")
        # Extract text from DOCX
        else:
            for i in range(start_page - 1, end_page):
                try:
                    para_text = doc.paragraphs[i].text
                    if para_text:
                        text += para_text + "\n"
                        print(f"Successfully extracted text from paragraph {i+1}")
                    else:
                        print(f"Warning: No text found in paragraph {i+1}")
                except Exception as e:
                    print(f"Error processing paragraph {i+1}: {str(e)}")

        if not text.strip():
            raise ValueError("No text content found in the specified page range")

        print(f"Extracted text length: {len(text)} characters")
        lines = text.splitlines()
        print(f"Number of lines: {len(lines)}")

        result = []
        current_term = None
        current_sub = None
        current_detail = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Äiá»u khoáº£n chÃ­nh
            if re.match(r"^Äiá»u\s+\d+[\.:]", line):
                if current_term:
                    if current_sub:
                        if current_detail:
                            current_sub["details"].append(current_detail)
                            current_detail = None
                        current_term["sub_items"].append(current_sub)
                        current_sub = None
                    result.append(current_term)
                current_term = {"title": line, "sub_items": []}

            # Má»¥c 1.1, 2.3,...
            elif re.match(r"^\d+\.\d+", line):
                if current_sub:
                    if current_detail:
                        current_sub["details"].append(current_detail)
                        current_detail = None
                    current_term["sub_items"].append(current_sub)
                current_sub = {"title": line, "details": []}
                current_detail = None

            # Má»¥c a), b), c)...
            elif re.match(r"^[a-zA-Z]\)", line):
                if current_detail:
                    current_sub["details"].append(current_detail)
                current_detail = {"title": line, "sub_details": []}

            # Má»¥c i), ii), iii)...
            elif re.match(r"^(i{1,3}|iv|v|vi|vii|viii|ix|x)\)", line, re.IGNORECASE):
                if current_detail:
                    current_detail["sub_details"].append(line)
                elif current_sub:
                    current_sub["details"].append({"title": line})

            # Má»¥c - gáº¡ch Ä‘áº§u dÃ²ng
            elif re.match(r"^- ", line):
                if current_detail:
                    current_detail["sub_details"].append(line)
                elif current_sub:
                    current_sub["details"].append({"title": line})

            # DÃ²ng ná»‘i tiáº¿p
            else:
                # Náº¿u Ä‘ang á»Ÿ cáº¥p sÃ¢u nháº¥t (i) hoáº·c -), ná»‘i vÃ o dÃ²ng cuá»‘i
                if current_detail and current_detail.get("sub_details"):
                    current_detail["sub_details"][-1] += " " + line
                # Náº¿u Ä‘ang á»Ÿ cáº¥p a)
                elif current_detail:
                    current_detail["title"] += " " + line
                # Náº¿u chá»‰ cÃ³ cáº¥p 1.1
                elif current_sub:
                    current_sub["title"] += " " + line

        # ÄÃ³ng cÃ¡c khá»‘i cÃ²n láº¡i
        if current_detail:
            current_sub["details"].append(current_detail)
        if current_sub:
            current_term["sub_items"].append(current_sub)
        if current_term:
            result.append(current_term)

        if not result:
            raise ValueError("No structured content found in the text")

        print(f"Found {len(result)} terms in the text")
        return result
    except Exception as e:
        raise Exception(f"Error processing file: {str(e)}")

# terms = extract_structured_terms("test.pdf", 6, 48)
# with open("output.json", "w", encoding="utf-8") as f:
#     json.dump(terms, f, ensure_ascii=False, indent=2)